{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3354767b",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b085d4-18b9-4e51-9fcc-b4e42de52e22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stable diffusion 사용되는 모델들\n",
    "# text-encoder와 text-tokenizer는 local 환경에 저장하고, 학습시에불러옵니다.\n",
    "\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "import logging\n",
    "import pandas as pd\n",
    "from torch import nn,Tensor\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os, json\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
    "from diffusers import UniPCMultistepScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from transformers.optimization import AdamW\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"/home/sangyeob/dev/d2d/5-K_stable_diffusion/tokenizer\")\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n",
    "scheduler = UniPCMultistepScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d99a97",
   "metadata": {},
   "source": [
    "## tokenizer\n",
    "\n",
    "encoder에 한국어 데이터를 학습시키고는 싶은데, 기존에 사용되는 cliptokenizer의 vocab을 보니 한국어가 없었습니다. clip vocab으로 한국어가 잘 tokenize 될 수 있는지를 확인하고, 없는 경우는 한국어가 추가된 tokenizer를 사용해야 했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91c6b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CLIPTokenizer.from_pretrained(\"/home/sangyeob/dev/d2d/5-K_stable_diffusion/tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be05cccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_e = 'hi, i\\'m korean'\n",
    "print(tokenizer.tokenize(text_e))\n",
    "text_k = '안녕하세요'\n",
    "print(tokenizer.tokenize(text_k))\n",
    "text_k = '요세하녕안'\n",
    "print(tokenizer.tokenize(text_k))\n",
    "text_k = '안'\n",
    "print(tokenizer.tokenize(text_k))\n",
    "text_k = '녕'\n",
    "print(tokenizer.tokenize(text_k))\n",
    "text_k = '하'\n",
    "print(tokenizer.tokenize(text_k))\n",
    "text_k = '세'\n",
    "print(tokenizer.tokenize(text_k))\n",
    "text_k = '요'\n",
    "print(tokenizer.tokenize(text_k))\n",
    "\n",
    "text_j = 'おはよう'\n",
    "print(tokenizer.tokenize(text_j))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8377d8",
   "metadata": {},
   "source": [
    "한국어 문장을 토큰화를 해보니, 영어처럼 형태소 단위로 토큰화를 하진 않지만 한글자씩 제대로 맵핑이 되고는 있는 것 같습니다. 어떤 원리로 저 글자들로 맵핑이 되는지는 몰라서 찜찜하지만 시간이 많지 않기 때문에, 또한 한국어 dataset을 추가적으로 학습한 clip model의 vocabs 를 봤는데도 한국어는 따로 존재하지 않았기 때문에 그냥 진행하기로 했습니다.\n",
    "\n",
    "1) 일단 진행\n",
    "2) 결과가 안나오면 한국어 tokenizer를 더 공부해서 추가. \n",
    "\n",
    "로 진행하기로 했습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3c5842",
   "metadata": {},
   "source": [
    "## Text Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0f3c69",
   "metadata": {},
   "source": [
    "text_encoder 역시 허깅페이스 CompVis/stable-diffusion-v1-4의 pretrained ClipTextModel을 사용하지만 편의를 위해 local 환경에 저장해두고 불러왔습니다. ClipTextModel은 처음본 모델이기 때문에 어떤 구조로 되어 있는지 summary를 했습니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3ee955",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = CLIPTextModel.from_pretrained(\"/home/sangyeob/dev/d2d/5-K_stable_diffusion/text_encoder\")\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(text_encoder, input_size = (2, 77), dtypes=[torch.int], depth=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c41eda3",
   "metadata": {},
   "source": [
    "기본적인 transformer encoder 구조인 것 같고, embedding 후의, attention layer가 여러개 있습니다. input size = (2, 77), output size = (batch_size, 77, 768)이고 77이 max_length, 768이 transformer의 hidden_dim 인 것 같습니다. Encoder layer가 12개나 되고 파라미터도 꽤나 많은 것으로 보아 학습시간이나 batch_size를 잘 조정해야 할 것 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9ce0ba",
   "metadata": {},
   "source": [
    "## loss\n",
    "\n",
    "(영어 text, 한국어 text)를 입력으로 받고, tokenizer와 encoder를 지난 후에 각각의 embedding이 유사해지도록 학습을 할 계획입니다. 다만, 영어 text의 embedding이 많이 바뀌면 안되기 때문에 이에 대한 parameter를 추가합니다\n",
    "\n",
    "최대한 학습전의 embedding을 해치지 않는 방향으로\n",
    "\n",
    "$L = cos_similarity(text_e, text_k) + cos_similiar$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b6bed8",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baedc918",
   "metadata": {},
   "source": [
    "1. 한국어-영어 번역 말뭉치(기술과학) \n",
    "2. 텍스트 한국어-영어 번역 말뭉치(사회과학) \n",
    "3. 텍스트 한국어-영어 번역(병렬) 말뭉치\n",
    "4. 방송콘텐츠 한국어-영어 번역 말뭉치\n",
    "\n",
    "AI hub에서 4가지 한국어/영어 번역 데이터셋을 사용했습니다. 데이터가 너무커서 로컬 환경에서 데이터셋을 정리헀습니다.\n",
    "\n",
    "총 4,893,326개의 이미지가 있었지만 약 25만개의 text pair만을 데이터로 사용하기로 했습니다.\n",
    "그 이유는 한국말-모델 KoCLIP 에서는 약 82783개의 image와 텍스트를 사용했는데, 이 프로젝트에서는 image encoder 없이 text encoder만 학습을 시킬 것이기 때문에 좀 더 적은 데이터로도 가능할 것 같다고 생각했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6823fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/sangyeob/dev/d2d/5-K_stable_diffusion/dataset'   \n",
    "\n",
    "data = []\n",
    "\n",
    "for dir in sorted(os.listdir(data_dir)) : \n",
    "    if dir.startswith('.') or dir.endswith('ipynb'): continue\n",
    "    filepath = data_dir + '/' + dir + '/data.json'\n",
    "    \n",
    "    \n",
    "    with open(filepath, 'r') as f :\n",
    "        js = json.loads(f.read())\n",
    "        data += js\n",
    "        \n",
    "    print(f'{dir} : {len(js)}개')    \n",
    "    \n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83848be2",
   "metadata": {},
   "source": [
    "Processing은 다음과 같이 진행합니다.\n",
    "\n",
    "1. duplicates 삭제\n",
    "\n",
    "2. null 값 삭제\n",
    "\n",
    "3. 기존 stable diffusion의 max_length인 77을 넘어가는 텍스트들 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8705d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 duplicate 삭제\n",
    "x = df.sample(2400000)\n",
    "x = x.drop_duplicates()\n",
    "print(f\"중복된 데이터를 제거한 데이터셋 개수 : {len(x)}\")\n",
    "\n",
    "#2 null 값 삭제\n",
    "x_null = x.isnull()\n",
    "print(f\"영어 텍스트 null 개수: {len(x_null.loc[x_null['english'] == True])}\")\n",
    "print(f\"한국어 텍스트 null 개수: {len(x_null.loc[x_null['korea'] == True])}\")\n",
    "x  = x.dropna(axis=0, how='any')\n",
    "print(f\"null값을 제거한 데이터셋 개수 : {len(x)}\")\n",
    "\n",
    "#3 10만개 sampling 후, max length 초과 텍스트들 제거제거\n",
    "# x_sampled = x.sample(100000)\n",
    "\n",
    "x['eng_tokenized'] = x['english'].apply(lambda x : len(tokenizer(x).input_ids) <= 77)\n",
    "x['kor_tokenized'] = x['korea'].apply(lambda x : len(tokenizer(x).input_ids) <= 77)\n",
    "x = x[x['eng_tokenized'] & x['kor_tokenized']]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae7c50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_100000 = x\n",
    "df_100000.to_csv('/home/sangyeob/dev/d2d/5-K_stable_diffusion/df_1000000.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e37824",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_100000 = pd.read_csv('/home/sangyeob/dev/d2d/5-K_stable_diffusion/df_100000.csv')\n",
    "df_100000\n",
    "\n",
    "df_100000['eng_tokenized'] = df_100000['english'].apply(lambda x : tokenizer(x, padding=\"max_length\", max_length=77, truncation=True, return_tensors='pt').input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f881a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = CLIPTextModel.from_pretrained(\"/home/sangyeob/dev/d2d/5-K_stable_diffusion/text_encoder\")\n",
    "text_encoder.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe407e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnKoDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=77):\n",
    "        self.text_e = tokenizer(list(df['english']), padding=\"max_length\", max_length=max_length, truncation=True, return_tensors='pt').input_ids\n",
    "        self.text_k = tokenizer(list(df['korea']), padding=\"max_length\", max_length=max_length, truncation=True, return_tensors='pt').input_ids\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_e)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text_e[idx], self.text_k[idx]\n",
    "\n",
    "def dataset_split(dataset, ratio):\n",
    "    train_size = int(ratio * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    return train_dataset, val_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5951657",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "clip training code와 과제4 학습코드를 참고하여 작성하였습니다. (https://github.com/Bing-su/train_ml_clip) \n",
    "\n",
    "\n",
    "input으로 받은 영어텍스트와 한국어 텍스트를 각각 ClipEncoder에 넣어주고, 결과로 나온 두개의 임베딩이 비슷해지도록 학습을 해볼 예정입니다.\n",
    "\n",
    "영어 텍스트와 한국어 텍스트는 같은 의미의 문장이므로, 두 임베딩은 유사한 의미를 가져야합니다. 이를 어떻게 표현할 수 있을까 많은 고민을 해봤는데, \n",
    "\n",
    "1. cosine similarity : 두 임베딩 벡터 간의 cosine similarity 구합니다. 의미가 유사할수록 높은 값을 가져야 합니다.\n",
    "\n",
    "2. Mean Squared Error : 두 임베딩 벡터 간의 수치가 비슷해지도록 학습이 됩니다.\n",
    "\n",
    "\n",
    "embedding(text_e)와 embedding(text_k)와의 cosine similarity를 loss로 줍니다.\n",
    "\n",
    "하지만, 학습과정에서 영어 텍스트에 대한 embedding이 너무 많이 변해버리면 이후의 unet에 text_embedding으로 들어갈때 이미지 결과가 크게 변할 수도 있기 때문에 이 너무 많은 값이 변하지 않기 위한 정규화 파라미터 또는 mse를 넣어줄 것입니다.\n",
    "\n",
    "우선 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095af507",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder = CLIPTextModel.from_pretrained(\"/home/sangyeob/dev/d2d/5-K_stable_diffusion/text_encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ae135c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = 'we are all precious.'\n",
    "text_2 = '우리는 모두 소중한 존재입니다.'\n",
    "\n",
    "\n",
    "text_e1 = tokenizer(text_1, padding=\"max_length\", max_length=77, truncation=True, return_tensors='pt').input_ids\n",
    "text_e2 = tokenizer(text_2, padding=\"max_length\", max_length=77, truncation=True, return_tensors='pt').input_ids\n",
    "text_k1 = tokenizer(\"고양이와 개\", padding=\"max_length\", max_length=77, truncation=True, return_tensors='pt').input_ids\n",
    "\n",
    "print(text_e1.shape)\n",
    "v1 = text_encoder(input_ids = text_e1)[0]\n",
    "v2 = text_encoder(input_ids = text_e2)[0]\n",
    "v3 = text_encoder(input_ids = text_k1)[0]\n",
    "\n",
    "\n",
    "mse_loss = nn.MSELoss()\n",
    "print(v1.shape)\n",
    "print(mse_loss(v1, v2))\n",
    "print(mse_loss(v1, v3))\n",
    "print(mse_loss(v2, v3))\n",
    "\n",
    "cos_loss = nn.CosineEmbeddingLoss()\n",
    "print(cos_loss(v1.reshape(1, -1), v2.reshape(1, -1), target= torch.ones(1)))\n",
    "print(cos_loss(v1.reshape(1, -1), v3.reshape(1, -1), target= torch.ones(1)))\n",
    "print(cos_loss(v2.reshape(1, -1), v3.reshape(1, -1), target= torch.ones(1)))\n",
    "\n",
    "# 한국말끼리는 cos simmilarity가 높은 모양"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4651612b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import logging\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch import nn,Tensor\n",
    "import numpy as np\n",
    "import os, json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "class EnKoDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_length=77):\n",
    "        self.text_e = tokenizer(list(df['english']), padding=\"max_length\", max_length=max_length, truncation=True, return_tensors='pt').input_ids\n",
    "        self.text_k = tokenizer(list(df['korea']), padding=\"max_length\", max_length=max_length, truncation=True, return_tensors='pt').input_ids\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text_e)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.text_e[idx], self.text_k[idx]\n",
    "\n",
    "def dataset_split(dataset, ratio):\n",
    "    train_size = int(ratio * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "\n",
    "\n",
    "def _validate(model, dev_dataloader, device, logger, global_step, loss_type, initial_embed_e_val):\n",
    "    \n",
    "    loss_fn_mse = nn.MSELoss()\n",
    "    loss_fn_cos = nn.CosineEmbeddingLoss()\n",
    "    loss_list = []\n",
    "    \n",
    "    model.eval() \n",
    "    \n",
    "    for step_index, batch_data in tqdm(enumerate(dev_dataloader), f\"[Eval] Valid\\t\", total=len(dev_dataloader)):\n",
    "        with torch.no_grad():\n",
    "            text_e, text_k = batch_data\n",
    "            batch_size = text_e.shape[0]\n",
    "            \n",
    "            texts = torch.cat((text_e, text_k), dim=0).to('cuda')\n",
    "            embeds = model(texts)[0]\n",
    "            embed_e, embed_k = embeds[:batch_size], embeds[batch_size:]\n",
    "        \n",
    "            if loss_type == \"mse\" : \n",
    "                loss = loss_fn_mse(embed_e, embed_k) \n",
    "                loss += loss_fn_mse(embed_e, initial_embed_e_val[step_index, :batch_size]) \n",
    "                loss += loss_fn_mse(embed_k, initial_embed_e_val[step_index, :batch_size])\n",
    "            elif loss_type == \"cos\" : \n",
    "                loss = loss_fn_cos(embed_e.reshape(batch_size, -1), embed_k.reshape(batch_size, -1), torch.ones(batch_size).to(device))\n",
    "                loss += loss_fn_cos(embed_e.reshape(batch_size, -1), initial_embed_e_val[step_index, :batch_size].reshape(batch_size, -1), torch.ones(batch_size).to(device))\n",
    "                loss += loss_fn_cos(embed_k.reshape(batch_size, -1), initial_embed_e_val[step_index, :batch_size].reshape(batch_size, -1), torch.ones(batch_size).to(device))\n",
    "                \n",
    "            loss_value = loss.cpu().clone().detach().numpy()\n",
    "            loss_list.append(loss_value)\n",
    "\n",
    "    mean_loss = np.mean(loss_list)\n",
    "    model.train()\n",
    "\n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "def train(model, optimizer, scheduler, train_dataloader, val_dataloader, device, loss_type, batch_size, max_length, hidden_dim):\n",
    "    \n",
    "    model.to(device)  \n",
    "    model.train() \n",
    "    \n",
    "    loss_list_between_log_interval = []\n",
    "    loss_fn_mse = nn.MSELoss()\n",
    "    loss_fn_cos = nn.CosineEmbeddingLoss()\n",
    "    \n",
    "    initial_embed_e = torch.zeros((step_per_epoch, batch_size, max_length, hidden_dim), device = device)\n",
    "    initial_embed_e_val = torch.zeros((step_per_epoch_val, batch_size, max_length, hidden_dim), device = device)\n",
    "    loss_value = 0\n",
    "    \n",
    "    logger.info('\\tinit validation dataset..start')\n",
    "    \n",
    "    for step_index, batch_data in tqdm(enumerate(val_dataloader), f\"[Eval] Valid\\t\", total=len(val_dataloader)) : \n",
    "        with torch.no_grad():\n",
    "            text_e, _ = batch_data\n",
    "            embed_e = model(text_e.to(device))[0]\n",
    "            initial_embed_e_val[step_index, :embed_e.shape[0]] = embed_e.clone().detach()\n",
    "    \n",
    "    logger.info('\\tinit validation dataset..end')\n",
    "    \n",
    "    for epoch_id in range(epochs):             \n",
    "        for step_index, batch_data in tqdm(enumerate(train_dataloader), f\"[TRAIN] Epoch_{epoch_id}\\t\", total=len(train_dataloader)):\n",
    "            \n",
    "            global_step = len(train_dataloader) * epoch_id + step_index + 1\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            text_e, text_k = batch_data\n",
    "            batch_size = text_e.shape[0]\n",
    "            \n",
    "            texts = torch.cat((text_e, text_k), dim=0).to('cuda')\n",
    "            embeds = model(texts)[0]\n",
    "            embed_e, embed_k = embeds[:batch_size], embeds[batch_size:]\n",
    "            \n",
    "            if epoch_id == 0 : initial_embed_e[step_index,:batch_size] = embed_e.clone().detach()\n",
    "                \n",
    "            if loss_type == \"mse\" : \n",
    "                loss = loss_fn_mse(embed_e, embed_k) + loss_fn_mse(embed_e, initial_embed_e[step_index,:batch_size]) + loss_fn_mse(embed_k, initial_embed_e[step_index,:batch_size])\n",
    "            elif loss_type == \"cos\" : \n",
    "                loss = loss_fn_cos(embed_e.reshape(batch_size, -1), embed_k.reshape(batch_size, -1), torch.ones(batch_size).to(device))\n",
    "                loss += loss_fn_cos(embed_e.reshape(batch_size, -1), initial_embed_e[step_index,:batch_size].reshape(batch_size, -1), torch.ones(batch_size).to(device))\n",
    "                loss += loss_fn_cos(embed_k.reshape(batch_size, -1), initial_embed_e[step_index,:batch_size].reshape(batch_size, -1), torch.ones(batch_size).to(device))\n",
    "            \n",
    "            loss.backward()\n",
    "            loss_value = loss.cpu().clone().detach().numpy()\n",
    "            loss_list_between_log_interval.append(loss_value)\n",
    "            \n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "        mean_loss = np.mean(loss_list_between_log_interval)\n",
    "        loss_list_between_log_interval.clear()\n",
    "        \n",
    "        logger.info(f\"\\ttrain_epoch:{epoch_id}\\t global_step:{global_step} loss:{mean_loss:.4f}\")\n",
    "        \n",
    "        val_loss = _validate(model, val_dataloader, device, logger, global_step, loss_type, initial_embed_e_val)\n",
    "        logger.info(f\"\\tvalidation\\t global_step:{global_step} loss:{val_loss:.4f}\")\n",
    "\n",
    "    # 모델 저장 코드\n",
    "    state_dict = model.state_dict()\n",
    "    model_path = os.path.join('/home/sangyeob/dev/d2d/5-K_stable_diffusion', f\"k-clip_encoder_{loss_type}.pth\")\n",
    "    torch.save(state_dict, model_path)\n",
    "    logger.info(\n",
    "        f\"saved at {model_path}\"\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5828a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CLIPTokenizer.from_pretrained(\"/home/sangyeob/dev/d2d/5-K_stable_diffusion/tokenizer\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"/home/sangyeob/dev/d2d/5-K_stable_diffusion/text_encoder\")\n",
    "df_100000 = pd.read_csv('/home/sangyeob/dev/d2d/5-K_stable_diffusion/df_100000.csv')\n",
    "\n",
    "batch_size = 8 \n",
    "dataset_df = df_100000.sample(1000)\n",
    "\n",
    "logger = logging.getLogger()  \n",
    "logger.setLevel(logging.INFO)  \n",
    "\n",
    "logging.info(\"\\tdataset loading..\")\n",
    "start_time = time.time()\n",
    "dataset = EnKoDataset(dataset_df, tokenizer)\n",
    "train_dataset, val_dataset = dataset_split(dataset, 0.8)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "logging.info(f\"\\tdataset loaded.. {(time.time() - start_time):.2f} passed\")\n",
    "\n",
    "# Config\n",
    "epochs = 10  \n",
    "learning_rate = 1e-6\n",
    "weight_decay = 1e-4\n",
    "save_interval = 1000 \n",
    "grad_clip = 1.0\n",
    "batch_size = 16\n",
    "\n",
    "# optimizer\n",
    "param_optimizer = list(text_encoder.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(\n",
    "        nd in n for nd in no_decay)], 'weight_decay': weight_decay},\n",
    "    {'params': [p for n, p in param_optimizer if any(\n",
    "        nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate, correct_bias=False)\n",
    "\n",
    "# scheduler\n",
    "step_per_epoch = len(train_dataloader)\n",
    "num_total_steps = step_per_epoch * epochs\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    learning_rate,\n",
    "    total_steps=num_total_steps\n",
    ")\n",
    "\n",
    "# scheduler = get_cosine_schedule_with_warmup(\n",
    "#     optimizer, \n",
    "#     num_warmup_steps = num_total_steps * 0.1, \n",
    "#     num_training_steps = num_total_steps\n",
    "# )\n",
    "\n",
    "step_per_epoch = len(train_dataloader)\n",
    "step_per_epoch_val = len(val_dataloader)\n",
    "total_train_dataset_len = len(train_dataset)\n",
    "total_valid_dataset_len = len(val_dataset)\n",
    "num_total_steps = step_per_epoch * epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9c4a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f'\\tstep_per_epoch : {step_per_epoch}')\n",
    "logging.info(f'\\tstep_per_epoch_val : {step_per_epoch_val}')\n",
    "logging.info(f'\\ttotal_dataset_len : {total_train_dataset_len} + {total_valid_dataset_len} = {total_train_dataset_len + total_valid_dataset_len}')\n",
    "logging.info(f'\\tnum_total_steps : {num_total_steps}')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "text_encoder = train(model=text_encoder, \n",
    "                    optimizer=optimizer, \n",
    "                    scheduler=scheduler, \n",
    "                    train_dataloader=train_dataloader, \n",
    "                    val_dataloader=val_dataloader, \n",
    "                    device=device, \n",
    "                    loss_type=\"mse\", \n",
    "                    batch_size=32, \n",
    "                    max_length=77, \n",
    "                    hidden_dim=768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4630421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"/home/sangyeob/dev/d2d/5-K_stable_diffusion/tokenizer\")\n",
    "\n",
    "text_encoder_1 = CLIPTextModel.from_pretrained(\"/home/sangyeob/dev/d2d/5-K_stable_diffusion/text_encoder\")\n",
    "text_encoder_2 = CLIPTextModel.from_pretrained(\"/home/sangyeob/dev/d2d/5-K_stable_diffusion/text_encoder/1/encoder_5e-7_10000\")\n",
    "\n",
    "text_encoder_1.eval()\n",
    "text_encoder_2.eval()\n",
    "\n",
    "with torch.no_grad() : \n",
    "    \n",
    "    text_e = 'dog painted by pen'\n",
    "    text_k = '펜으로 그린 강아지'\n",
    "    token_e = tokenizer(text_e, padding=\"max_length\", max_length=77, truncation=True, return_tensors='pt').input_ids\n",
    "    token_k = tokenizer(text_k, padding=\"max_length\", max_length=77, truncation=True, return_tensors='pt').input_ids\n",
    "\n",
    "    init_embed_e = text_encoder_1(input_ids = token_e)[0]\n",
    "    init_embed_k = text_encoder_1(input_ids = token_k)[0]\n",
    "    \n",
    "    mse_loss = nn.MSELoss()\n",
    "    cos_loss = nn.CosineEmbeddingLoss()\n",
    "    \n",
    "    embed_e = text_encoder_2(input_ids = token_e)[0]\n",
    "    embed_k = text_encoder_2(input_ids = token_k)[0]\n",
    "    \n",
    "    print(f'학습 전 영어, 한국어 간 mse_loss : {mse_loss(init_embed_e, init_embed_k)}')\n",
    "    print(f'학습 후 영어, 한국어 간 mse_loss : {mse_loss(embed_e, embed_k)}')\n",
    "    print(f'원래 임베딩과 영어 텍스트 간 mse_loss : {mse_loss(embed_e, init_embed_e)}')\n",
    "    print(f'원래 임베딩과 한국어 텍스트 간 mse_loss : {mse_loss(embed_k, init_embed_e)}')\n",
    "    print(f'전체 mse_loss : {mse_loss(embed_e, embed_k) + mse_loss(embed_e, init_embed_e) + mse_loss(embed_k, init_embed_e)}\\n')\n",
    "\n",
    "    init_embed_e = init_embed_e.reshape(1, -1)\n",
    "    init_embed_k = init_embed_k.reshape(1, -1)\n",
    "    embed_e = embed_e.reshape(1, -1)\n",
    "    embed_k = embed_k.reshape(1, -1)\n",
    "    target = torch.ones(1)\n",
    "    \n",
    "    print(f'학습 전 영어, 한국어 간 cos_loss : {cos_loss(init_embed_e, init_embed_k, target)}')\n",
    "    print(f'학습 후 영어, 한국어 간 cos_loss : {cos_loss(embed_e, embed_k, target)}')\n",
    "    print(f'원래 임베딩과 영어 텍스트 간 cos_loss : {cos_loss(embed_e, init_embed_e, target)}')\n",
    "    print(f'원래 임베딩과 한국어 텍스트 간 cos_loss : {cos_loss(embed_k, init_embed_e, target)}')\n",
    "    print(f'전체 cos_loss : {cos_loss(embed_e, embed_k, target) + cos_loss(embed_e, init_embed_e, target) + cos_loss(embed_k, init_embed_e, target)}')\n",
    "    \n",
    "# 한국말끼리는 cos simmilarity가 높은 모양\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30528e7-f877-416f-806c-f9e2cab43287",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
    "from diffusers import UniPCMultistepScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"/home/sangyeob/dev/d2d/5-K_stable_diffusion/text_encoder/1/encoder_5e-7_10000\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"/home/sangyeob/dev/d2d/5-K_stable_diffusion/tokenizer\")\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\")\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n",
    "scheduler = UniPCMultistepScheduler.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"scheduler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb4ff43-31bc-4312-951c-16e22c9c7c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vae = sum(p.numel() for p in vae.parameters() if p.requires_grad)\n",
    "n_text_encoder = sum(p.numel() for p in text_encoder.parameters() if p.requires_grad)\n",
    "n_unet = sum(p.numel() for p in unet.parameters() if p.requires_grad)\n",
    "n_total = n_vae + n_text_encoder + n_unet\n",
    "\n",
    "n_vae = format(n_vae, ',')\n",
    "n_text_encoder = format(n_text_encoder, ',')\n",
    "n_unet = format(n_unet, ',')\n",
    "n_total = format(n_total, ',')\n",
    "\n",
    "print(f'vae : {n_vae}')\n",
    "print(f'text_encoder : {n_text_encoder}')\n",
    "print(f'unet : {n_unet}')\n",
    "print(f'total : {n_total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fea67f7-72f4-4587-8d60-15b3171ab3d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch_device = \"cuda\"\n",
    "unet.to(torch_device)\n",
    "text_encoder.to(torch_device)\n",
    "vae.to(torch_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8654bc-80c0-47f2-ac38-cf89a5d9f237",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# input 준비\n",
    "text = [\"yellow car in front of brick house\", \"강아지\"]\n",
    "height = 512  # default height of Stable Diffusion\n",
    "width = 512  # default width of Stable Diffusion\n",
    "num_inference_steps = 25  # Number of denoising steps\n",
    "guidance_scale = 7.5  # Scale for classifier-free guidance\n",
    "generator = torch.manual_seed(0)  # Seed generator to create the inital latent noise\n",
    "batch_size = len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e09162-cc3c-4660-9a66-6626c1e39b1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_input = tokenizer(\n",
    "    text, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "print(text_input.input_ids.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "    # text_embeddings = text_encoder(text_input.input_ids.to(torch_device), text_input.attention_mask.to(torch_device))[0]\n",
    "\n",
    "max_length = text_input.input_ids.shape[-1]\n",
    "uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "\n",
    "latents = torch.randn(\n",
    "    (batch_size, unet.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    ")\n",
    "latents = latents.to(torch_device)\n",
    "latents = latents * scheduler.init_noise_sigma\n",
    "\n",
    "print(text_embeddings.shape, latents.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf5e8c0-0377-4f14-b76b-fc0c57932402",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "for t in tqdm(scheduler.timesteps):\n",
    "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "    latent_model_input = torch.cat([latents] * 2)\n",
    "\n",
    "    latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n",
    "\n",
    "    # predict the noise residual\n",
    "    with torch.no_grad():\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "    # perform guidance\n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    # compute the previous noisy sample x_t -> x_t-1\n",
    "    latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d274ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latents.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dfee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = 1 / 0.18215 * latents\n",
    "\n",
    "with torch.no_grad():\n",
    "    image = vae.decode(latents).sample\n",
    "\n",
    "print(type(image))\n",
    "image = (image / 2 + 0.5).clamp(0, 1)\n",
    "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "images = (image * 255).round().astype(\"uint8\")\n",
    "pil_images = [Image.fromarray(image) for image in images]\n",
    "\n",
    "# display images\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].imshow(pil_images[0])\n",
    "ax[1].imshow(pil_images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0ef559-0aab-40e7-97a5-93e0e968ca04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "latents = 1 / 0.18215 * latents\n",
    "\n",
    "with torch.no_grad():\n",
    "    image = vae.decode(latents).sample\n",
    "\n",
    "print(type(image))\n",
    "image = (image / 2 + 0.5).clamp(0, 1)\n",
    "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "images = (image * 255).round().astype(\"uint8\")\n",
    "pil_images = [Image.fromarray(image) for image in images]\n",
    "\n",
    "# display images\n",
    "fig, ax = plt.subplots(1,2)\n",
    "ax[0].imshow(pil_images[0])\n",
    "ax[1].imshow(pil_images[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99744169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e4e672",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2d",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
